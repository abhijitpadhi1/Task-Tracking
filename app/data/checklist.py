"""Static checklist metadata for all stages, repositories, and tasks."""

from __future__ import annotations

from typing import Any, Dict, List

StagePayload = Dict[str, Any]
RepositoryPayload = Dict[str, Any]
TaskPayload = Dict[str, Any]


def _task(repo: str, slug: str, title: str, description: str, ordering: int) -> TaskPayload:
    return {
        "id": f"{repo}-{slug}",
        "title": title,
        "description": description,
        "ordering": ordering,
    }


def _repo(
    stage: str,
    slug: str,
    title: str,
    description: str,
    tasks: List[TaskPayload],
    ordering: int,
) -> RepositoryPayload:
    return {
        "id": slug,
        "stage_id": stage,
        "title": title,
        "description": description,
        "ordering": ordering,
        "tasks": tasks,
    }


STAGES: List[StagePayload] = [
    {
        "id": "stage-1",
        "title": "Stage 1 — Data & ML Foundations",
        "description": "Core Python through production ML pipelines to prove end-to-end competence.",
        "ordering": 1,
        "repositories": [
            _repo(
                "stage-1",
                "python-fundamentals",
                "python-fundamentals",
                "Core Python, OOP, generators, decorators, error handling, and unit tests. Why: recruiters check whether you can code cleanly.",
                [
                    _task("python-fundamentals", "loops-functions", "Write scripts demonstrating loops & functions", "Cover for/while loops, higher-order functions, and argument patterns.", 1),
                    _task("python-fundamentals", "comprehensions", "Build list/dict/set comprehension examples", "Showcase readable transformations and filtering.", 2),
                    _task("python-fundamentals", "file-ops", "Practice file operations", "Read, write, and append files using context managers.", 3),
                    _task("python-fundamentals", "error-handling", "Implement structured error handling", "Use try/except/else/finally and custom exceptions.", 4),
                    _task("python-fundamentals", "decorators", "Implement decorators", "Demonstrate function decorators with arguments and metadata preservation.", 5),
                    _task("python-fundamentals", "generators", "Create generators", "Yield-based pipelines plus generator expressions.", 6),
                    _task("python-fundamentals", "context-managers", "Author custom context managers", "Use both class-based and @contextmanager patterns.", 7),
                    _task("python-fundamentals", "oop-inheritance", "Build OOP hierarchy", "Classes, inheritance, and composition examples.", 8),
                    _task("python-fundamentals", "abstract-classes", "Use abstract base classes", "Define interfaces via abc.ABC and enforce overrides.", 9),
                    _task("python-fundamentals", "static-class-methods", "Implement static & class methods", "Demonstrate factory constructors and utility helpers.", 10),
                    _task("python-fundamentals", "pytest-suite", "Add pytest unit tests", "Ensure coverage of modules authored above.", 11),
                ],
                1,
            ),
            _repo(
                "stage-1",
                "math-for-ml",
                "math-for-ml",
                "Linear algebra, calculus, optimization, KL divergence, and information theory notebooks for ML intuition.",
                [
                    _task("math-for-ml", "dot-product", "Notebook: dot product & matrix multiplication", "Use NumPy to visualize vector operations.", 1),
                    _task("math-for-ml", "eigen-svd", "Notebook: eigenvalues, eigenvectors, and SVD", "Show geometric intuition plus numerical demos.", 2),
                    _task("math-for-ml", "gradients", "Notebook: gradients of simple functions", "Derive by hand and verify with autograd.", 3),
                    _task("math-for-ml", "optimization", "Notebook: basic optimization demos", "Implement gradient descent / momentum on toy surfaces.", 4),
                    _task("math-for-ml", "softmax-crossentropy", "Notebook: softmax & cross-entropy", "Plot surfaces and verify derivatives.", 5),
                    _task("math-for-ml", "entropy-kl", "Notebook: entropy, perplexity, KL divergence", "Compare distributions and compute metrics.", 6),
                ],
                2,
            ),
            _repo(
                "stage-1",
                "ml-from-scratch",
                "ml-from-scratch",
                "Implement classic ML algorithms with only NumPy to build an sklearn-mini toolkit.",
                [
                    _task("ml-from-scratch", "linear-regression", "Implement linear regression", "Closed-form and gradient descent solutions with .fit/.predict/.evaluate.", 1),
                    _task("ml-from-scratch", "logistic-regression", "Implement logistic regression", "Binary classifier complete with metrics.", 2),
                    _task("ml-from-scratch", "softmax-classifier", "Implement softmax classifier", "Multiclass cross-entropy pipeline.", 3),
                    _task("ml-from-scratch", "knn", "Implement k-Nearest Neighbors", "Support distance metrics and evaluation.", 4),
                    _task("ml-from-scratch", "decision-tree", "Implement decision tree", "Gini/entropy splits and depth control.", 5),
                    _task("ml-from-scratch", "random-forest", "Implement random forest", "Bootstrap aggregation plus feature sampling.", 6),
                    _task("ml-from-scratch", "svm", "Implement SVM (hard+soft margin)", "Train using quadratic programming or SMO.", 7),
                ],
                3,
            ),
            _repo(
                "stage-1",
                "ml-projects",
                "ml-projects",
                "Four dataset case studies (Titanic, home prices, churn, MNIST) each covering EDA through reporting.",
                [
                    _task("ml-projects", "titanic", "Titanic survival project", "EDA, feature engineering, baseline vs improved models, final report & saved model.", 1),
                    _task("ml-projects", "home-prices", "Home price prediction project", "Replicate the entire workflow including model tracking.", 2),
                    _task("ml-projects", "customer-churn", "Customer churn project", "Hold-out evaluation plus business-focused report.", 3),
                    _task("ml-projects", "mnist", "MNIST classification project", "Sklearn baseline and improved pipeline with persistence.", 4),
                ],
                4,
            ),
            _repo(
                "stage-1",
                "ml-pipeline-end-to-end",
                "ml-pipeline-end-to-end",
                "Production-grade pipeline: ingestion, preprocessing, training, evaluation, FastAPI, Docker, deployment.",
                [
                    _task("ml-pipeline-end-to-end", "etl", "Build ETL pipeline", "Automate raw data ingestion and validation.", 1),
                    _task("ml-pipeline-end-to-end", "preprocessing", "Develop preprocessing module", "Feature scaling, encoding, and artifacts.", 2),
                    _task("ml-pipeline-end-to-end", "training", "Author training script", "Reproducible training entrypoint with config.", 3),
                    _task("ml-pipeline-end-to-end", "evaluation", "Create evaluation module", "Generate metrics and comparison plots.", 4),
                    _task("ml-pipeline-end-to-end", "model-saving", "Persist model artifacts", "Use joblib and versioning strategy.", 5),
                    _task("ml-pipeline-end-to-end", "fastapi-endpoint", "Expose FastAPI endpoint", "Serve predictions with input validation.", 6),
                    _task("ml-pipeline-end-to-end", "docker", "Write Dockerfile", "Containerize training + inference.", 7),
                    _task("ml-pipeline-end-to-end", "deployment", "Deploy to Render/Railway", "Document deployment steps and health checks.", 8),
                ],
                5,
            ),
        ],
    },
    {
        "id": "stage-2",
        "title": "Stage 2 — Deep Learning Foundations",
        "description": "From NumPy neural nets to PyTorch workflows and vision/audio/text projects.",
        "ordering": 2,
        "repositories": [
            _repo(
                "stage-2",
                "neural-networks-from-scratch",
                "neural-networks-from-scratch",
                "NumPy-only neural network library including layers, activations, losses, optimizers, and MNIST training loop.",
                [
                    _task("neural-networks-from-scratch", "dense-layer", "Implement dense layer", "Forward/backward passes with parameter init.", 1),
                    _task("neural-networks-from-scratch", "activations", "Implement ReLU/Tanh/Sigmoid", "Vectorized forward and derivative computations.", 2),
                    _task("neural-networks-from-scratch", "losses", "Implement MSE & CrossEntropy losses", "Return scalar loss and gradients.", 3),
                    _task("neural-networks-from-scratch", "optimizers", "Implement SGD, Momentum, Adam", "Stateful updates with configurable hyperparameters.", 4),
                    _task("neural-networks-from-scratch", "backprop", "Wire full backpropagation", "Support arbitrary layer stacks with gradient checks.", 5),
                    _task("neural-networks-from-scratch", "training-loop", "Build training loop", "Mini-batching, logging, validation splits.", 6),
                    _task("neural-networks-from-scratch", "mnist-training", "Train small NN on MNIST", "Show metrics and saved weights.", 7),
                ],
                1,
            ),
            _repo(
                "stage-2",
                "pytorch-essentials",
                "pytorch-essentials",
                "PyTorch fundamentals: custom datasets, CNN/RNN models, metrics, checkpointing, and TensorBoard logging.",
                [
                    _task("pytorch-essentials", "dataset", "Write custom Dataset & DataLoader", "Handle augmentation and batching logic.", 1),
                    _task("pytorch-essentials", "cnn-classifier", "Implement CNN classifier", "Train/test pipeline with metrics.", 2),
                    _task("pytorch-essentials", "rnn-classifier", "Implement RNN/GRU classifier", "Sequence modeling with packed sequences.", 3),
                    _task("pytorch-essentials", "training-utils", "Build training loops with metrics", "Accuracy, F1, confusion matrix utilities.", 4),
                    _task("pytorch-essentials", "early-stopping", "Add early stopping & checkpointing", "Persist best weights and resume.", 5),
                    _task("pytorch-essentials", "tensorboard", "Integrate TensorBoard logging", "Loss/metric curves and sample visuals.", 6),
                ],
                2,
            ),
            _repo(
                "stage-2",
                "deep-learning-projects",
                "deep-learning-projects",
                "Applied DL suite: CIFAR-10 CNN, IMDB text classifier, UrbanSound8K 1D CNN with confusion matrices and training graphs.",
                [
                    _task("deep-learning-projects", "cifar10", "CNN on CIFAR-10", "Data augmentation, training curves, final metrics.", 1),
                    _task("deep-learning-projects", "imdb", "LSTM/GRU on IMDB", "Sequence preprocessing and evaluation plots.", 2),
                    _task("deep-learning-projects", "urbansound", "1D CNN for UrbanSound8K", "Audio feature extraction + confusion matrix.", 3),
                    _task("deep-learning-projects", "visualizations", "Add training graphs & confusion matrices", "Consistent reporting across projects.", 4),
                ],
                3,
            ),
            _repo(
                "stage-2",
                "transformer-from-scratch",
                "transformer-from-scratch",
                "Critical project: build Transformer components (attention, encoder/decoder, masking) and train a Shakespeare LM.",
                [
                    _task("transformer-from-scratch", "scaled-attention", "Implement scaled dot-product attention", "Include masking support.", 1),
                    _task("transformer-from-scratch", "multi-head", "Implement multi-head attention", "Projection matrices and concatenation.", 2),
                    _task("transformer-from-scratch", "positional-encoding", "Implement positional encodings", "Both sinusoidal and learned variants.", 3),
                    _task("transformer-from-scratch", "encoder-block", "Build encoder block", "Layer norm, residuals, feed-forward nets.", 4),
                    _task("transformer-from-scratch", "decoder-block", "Build decoder block", "Add causal mask and cross-attention.", 5),
                    _task("transformer-from-scratch", "lm-head", "Assemble GPT-style LM", "Stack blocks and add output projection.", 6),
                    _task("transformer-from-scratch", "shakespeare-training", "Train on Shakespeare dataset", "Demonstrate sampling outputs.", 7),
                ],
                4,
            ),
            _repo(
                "stage-2",
                "research-paper-implementations",
                "research-paper-implementations",
                "Paper re-creations: LeNet, ResNet, DenseNet, Seq2Seq, Attention with summaries, scripts, and result comparisons.",
                [
                    _task("research-paper-implementations", "lenet", "Implement LeNet", "Provide paper summary, architecture, training script.", 1),
                    _task("research-paper-implementations", "resnet", "Implement ResNet", "Residual blocks plus reported results.", 2),
                    _task("research-paper-implementations", "densenet", "Implement DenseNet", "Dense blocks, growth rate experiments.", 3),
                    _task("research-paper-implementations", "seq2seq", "Implement Seq2Seq", "Encoder-decoder with attention baseline.", 4),
                    _task("research-paper-implementations", "attention", "Implement attention mechanisms", "Compare with paper metrics.", 5),
                ],
                5,
            ),
        ],
    },
    {
        "id": "stage-3",
        "title": "Stage 3 — NLP & LLM Fundamentals",
        "description": "Tokenization, embeddings, language models, GPT-style training, and LoRA/RAG systems.",
        "ordering": 3,
        "repositories": [
            _repo(
                "stage-3",
                "nlp-foundations",
                "nlp-foundations",
                "Tokenization, Word2Vec, LSTM & GRU language models trained on small corpora.",
                [
                    _task("nlp-foundations", "bpe", "Implement BPE tokenization", "Train merges and encode/decode strings.", 1),
                    _task("nlp-foundations", "word2vec", "Implement Word2Vec (CBOW + SkipGram)", "Negative sampling and analogy tests.", 2),
                    _task("nlp-foundations", "lstm-lm", "Implement LSTM language model", "Train on mini dataset and evaluate perplexity.", 3),
                    _task("nlp-foundations", "gru-lm", "Implement GRU language model", "Compare against LSTM baseline.", 4),
                    _task("nlp-foundations", "training", "Train small models", "Document hyperparameters and results.", 5),
                ],
                1,
            ),
            _repo(
                "stage-3",
                "transformer-language-model",
                "transformer-language-model",
                "GPT-2 style causal transformer with sampling strategies, training loop, text generation, and checkpointing.",
                [
                    _task("transformer-language-model", "causal-transformer", "Implement causal transformer", "Stack decoder blocks with causal masks.", 1),
                    _task("transformer-language-model", "trainer", "Build training loop", "Handles batching, LR schedules, evaluation.", 2),
                    _task("transformer-language-model", "sampling", "Implement temperature & top-k sampling", "Expose CLI/API for generation.", 3),
                    _task("transformer-language-model", "generate", "Generate sample text", "Log checkpoints and qualitative outputs.", 4),
                    _task("transformer-language-model", "checkpoints", "Save & load checkpoints", "Resume training seamlessly.", 5),
                ],
                2,
            ),
            _repo(
                "stage-3",
                "lora-finetuning",
                "lora-finetuning",
                "Fine-tune GPT-2, LLaMA-3 8B, and Mistral 7B using PEFT/LoRA with 4/8-bit quantization for multiple tasks.",
                [
                    _task("lora-finetuning", "load-models", "Load models with transformers", "Prepare GPT-2, LLaMA-3, Mistral checkpoints.", 1),
                    _task("lora-finetuning", "quantization", "Apply 8-bit / 4-bit quantization", "Use bitsandbytes + QLoRA configs.", 2),
                    _task("lora-finetuning", "lora-setup", "Apply LoRA adapters (PEFT)", "Configure target modules and ranks.", 3),
                    _task("lora-finetuning", "gpt2-finetune", "Fine-tune GPT-2 small", "Target text classification & chat-style data.", 4),
                    _task("lora-finetuning", "llama3-finetune", "Fine-tune LLaMA-3 8B", "Instruction tuning workflow.", 5),
                    _task("lora-finetuning", "mistral-finetune", "Fine-tune Mistral 7B", "Chat and instruction tuning variant.", 6),
                    _task("lora-finetuning", "evaluation", "Evaluate improvements", "Report metrics for each task (classification/chat/instruction).", 7),
                ],
                3,
            ),
            _repo(
                "stage-3",
                "rag-system",
                "rag-system",
                "Full Retrieval-Augmented Generation system with embeddings, vector DB, retriever, generator, evaluation, API, and web UI.",
                [
                    _task("rag-system", "ingestion", "Create document ingestion pipeline", "Chunking, metadata capture, scheduling.", 1),
                    _task("rag-system", "embedding", "Implement embedding module", "Supports FAISS or Chroma ingestion.", 2),
                    _task("rag-system", "retriever", "Build retriever module", "Similarity search, scoring, filtering.", 3),
                    _task("rag-system", "generator", "Integrate generator model", "Wire GPT-2/LLaMA responses with retrieved context.", 4),
                    _task("rag-system", "evaluation", "Add evaluation module", "Measure accuracy & relevancy metrics.", 5),
                    _task("rag-system", "api", "Expose FastAPI server", "Serve query endpoint with latency logging.", 6),
                    _task("rag-system", "ui", "Add Streamlit UI", "Interactive chat + retrieved context view.", 7),
                ],
                4,
            ),
        ],
    },
    {
        "id": "stage-4",
        "title": "Stage 4 — Agents & Advanced LLM Work",
        "description": "Instruction tuning, quantization, agent tooling, and full-stack AI delivery.",
        "ordering": 4,
        "repositories": [
            _repo(
                "stage-4",
                "rlhf-mini",
                "rlhf-mini",
                "Mini InstructGPT pipeline covering SFT, reward modeling, PPO/DPO, and evaluation.",
                [
                    _task("rlhf-mini", "sft", "Perform supervised fine-tuning", "Seed policy model with curated data.", 1),
                    _task("rlhf-mini", "reward-model", "Build reward model", "Preference dataset + scoring network.", 2),
                    _task("rlhf-mini", "ppo-dpo", "Implement PPO or DPO", "Reinforcement learning loop with safety checks.", 3),
                    _task("rlhf-mini", "evaluation", "Evaluate with instructions", "Collect win-rates and qualitative analysis.", 4),
                ],
                1,
            ),
            _repo(
                "stage-4",
                "llm-quantization",
                "llm-quantization",
                "LLM efficiency repo: LLM.int8, QLoRA 4-bit, activation quantization, plus latency/memory/accuracy benchmarks.",
                [
                    _task("llm-quantization", "int8", "Implement 8-bit quantization (LLM.int8)", "Benchmark speed/memory.", 1),
                    _task("llm-quantization", "int4", "Implement 4-bit quantization (QLoRA)", "Compare quality to int8.", 2),
                    _task("llm-quantization", "activation", "Implement activation quantization", "Profile additional savings.", 3),
                    _task("llm-quantization", "benchmarks", "Benchmark latency, memory, accuracy", "Tabulate across models.", 4),
                ],
                2,
            ),
            _repo(
                "stage-4",
                "agentic-ai-projects",
                "agentic-ai-projects",
                "ReAct-style agents with tools, memory, multi-agent workflows using LangGraph, MCP, and planning systems.",
                [
                    _task("agentic-ai-projects", "react-agent", "Implement ReAct agent", "Tool reasoning loop using OpenAI-style prompts.", 1),
                    _task("agentic-ai-projects", "tools", "Add calculator, search, code executor tools", "Sandboxing + validation.", 2),
                    _task("agentic-ai-projects", "memory", "Implement short & long-term memory", "Vector DB for episodic recall.", 3),
                    _task("agentic-ai-projects", "planning", "Build planning & task decomposition", "LangGraph or MCP planner.", 4),
                    _task("agentic-ai-projects", "multi-agent", "Create multi-agent workflows", "Coordinator + specialist agents.", 5),
                ],
                3,
            ),
            _repo(
                "stage-4",
                "full-stack-ai-app",
                "full-stack-ai-app",
                "Flagship project: LoRA fine-tuning, RAG, agents, vector DB, FastAPI backend, authenticated frontend, Docker & deployment.",
                [
                    _task("full-stack-ai-app", "lora", "Integrate LoRA fine-tuning module", "Expose config & monitoring.", 1),
                    _task("full-stack-ai-app", "rag", "Embed RAG subsystem", "Pipelines for ingestion/retrieval.", 2),
                    _task("full-stack-ai-app", "agents", "Wire agent tools", "Planner + tool registry + evaluations.", 3),
                    _task("full-stack-ai-app", "backend", "Build FastAPI backend", "Auth, rate limits, orchestration.", 4),
                    _task("full-stack-ai-app", "frontend", "Create React/Streamlit frontend", "Authenticated dashboard with chat.", 5),
                    _task("full-stack-ai-app", "deployment", "Dockerize & deploy", "CI/CD plus cloud target instructions.", 6),
                ],
                4,
            ),
        ],
    },
    {
        "id": "stage-bonus",
        "title": "Bonus — Research-Level Extensions",
        "description": "Optional repositories for deeper research credibility.",
        "ordering": 5,
        "repositories": [
            _repo(
                "stage-bonus",
                "paper-reviews",
                "paper-reviews",
                "One-page summaries with equations, diagrams, and implementation notes for seminal papers.",
                [
                    _task("paper-reviews", "attention-is-all-you-need", "Review: Attention Is All You Need", "Summary, key formulas, diagram, implementation notes.", 1),
                    _task("paper-reviews", "distilbert", "Review: DistilBERT", "Compression insights and practical notes.", 2),
                    _task("paper-reviews", "gpt3", "Review: GPT-3", "Scaling laws and architecture diagram.", 3),
                    _task("paper-reviews", "switch-transformer", "Review: Switch Transformer", "MoE routing equations.", 4),
                    _task("paper-reviews", "lora", "Review: LoRA", "Low-rank adapters explanation and tips.", 5),
                    _task("paper-reviews", "llm-int8", "Review: LLM.int8", "Quantization math and constraints.", 6),
                    _task("paper-reviews", "rag", "Review: RAG", "Retriever-generator interface details.", 7),
                    _task("paper-reviews", "rlhf", "Review: RLHF", "Pipeline diagrams and pseudo-code.", 8),
                    _task("paper-reviews", "agents", "Review: Agents research", "Key agent architectures and protocols.", 9),
                ],
                1,
            ),
            _repo(
                "stage-bonus",
                "tiny-llm-training",
                "tiny-llm-training",
                "Train a 20–40M parameter GPT-2-sized model on TinyShakespeare, TinyStories, and a Wikipedia subset.",
                [
                    _task("tiny-llm-training", "architecture", "Build GPT-2 sized model (20–40M params)", "Document layer sizes & parameter counts.", 1),
                    _task("tiny-llm-training", "tinyshakespeare", "Train on TinyShakespeare", "Track loss curves and samples.", 2),
                    _task("tiny-llm-training", "tinystories", "Train on TinyStories", "Compare generalization and style.", 3),
                    _task("tiny-llm-training", "wikipedia", "Train on Wikipedia subset", "Handle tokenization and batching.", 4),
                    _task("tiny-llm-training", "loss-plots", "Plot loss curves", "Overlay datasets for comparison.", 5),
                    _task("tiny-llm-training", "text-generation", "Generate sample text", "Showcase outputs per dataset.", 6),
                ],
                2,
            ),
        ],
    },
]

